**Molecular descriptor prediction**
Given a molecular graph, around 160 different molecular descriptors are computed using RDKit and put together into a single vector. This will serve as a label for our first pre-training task. Given a molecular graph, the model will predict these descriptors. I have already implemented a first draft of this pretraining task.
While a lot of the other tasks listed here are designed to capture in depth structural knowledge about a molecule, molecular descriptor prediction is designed to learn chemical knowledge in a molecule, making it an essential part of any pre-training process. Some simple examples of such chemical descriptors include log and min and max partial charge. We also include a variety of topological and count descriptors (count the number of aliphatic heterocycles for example) in an effort to encode chemical domain knowledge into our model. By teaching our model to predict the number of aromatic rings or the number of hydrogen donors, we hope to give the model an understanding of such chemical concepts.

**Molecular Fingerprint Prediction**
In this task, the model will learn to predict standardized structural and or chemical fingerprints of a molecule from its graph representation. In the first draft of this task, I have it learning MACCS, Morgan, RDKit, and Avalon fingerprints, but this kind of redundancy is probably not needed, I think 1 or 2 structural representations will do.
The aim of this task is to encode a combination of the chemical and structural information of our data. The first type of molecular fingerprinting used deals primarily with the chemical information in molecules. In Morgan fingerprinting, the atom groups of a chemical are encoded into a binary vector, further embedding chemical domain knowledge into our model. The second type of fingerprinting we use, ECFP (Extended Connectivity Fingerprints) are designed to capture the topological and structural information of each molecule. By using a variety of such fingerprints, we hope to further enhance our models structural and chemical knowledge.

**Molecular Reconstruction**
 The latent representations from the CGTNN backbone are upsampled back into a full molecular graph (very computationally expensive). This generator will also be useful for downstream de-novo generation tasks. Currently, I have implemented the generator as a kind of graph variational autoencoder, but the code is still extremely messy. As part of this task, there would also eventually be a discriminator much like in any GAN. This discriminator will hopefully help enforce some sense of molecular rationality in the generator.  It might also be worth adding an explicit rationality penalty here, to make sure the generated graphs can be converted to SMILES or some other string representation, although this should be learned implicitly over time.
The main motivation behind this component is to further enforce a correlation between a molecular graph and its representation in latent space. Instead of using proxies like cluster pseudo labels, or chemical fingerprints, this task aims to enforce this correlation directly.

**Molecular Permutation Prediction** This is a pre-training task I came up with to enforce molecular rationality within the model. Given some of the other tasks, I’m unsure if this is actually necessary and it could be a good idea to reduce computational expense. In this task, the node (or edge matrix) of a graph is split up into equally sized, roughly square chunks. A shuffling is then applied to this matrix with a desired hamming distance. Given the shuffled matrix, the model will learn to predict what way the matrix was shuffled. For 4 chunks say, it might predict the shuffle as a vector like [1, 0, 3, 2]. This was motivated by an analogous pre-training task in computer vision, where models predict how an image was shuffled. As I write this, I’m working on the first draft of the code for this task.
(As far as I know, this task is new when applied to graphs) The motivation behind this task is to enforce a sense of topological and structural rationality within our model. By predicting HOW a molecular graph was rearranged our model will learn what makes a chemically “rational” molecule different from a chemically “irrational molecule”.

**Mask Fill In**
 The idea here is to take a molecular graph and mask out some portion of it. Given the latent representation of the masked model, and some topological/chemical information or fingerprint of the original molecule, a decoder will attempt to predict the masked portion of the original molecule. I have not begun to write any of the code for this task. This is different from similar techniques which use contrastive learning on masked and unmasked molecules as a pre-training strategy. Such a technique could sometimes put functionally different molecules right next to one another in representation space. Through avoiding contrastive learning when dealing with masked molecules, we hopefully won’t have that problem. Many other studies also have a variation of this task where the properties of masked areas are predicted directly from a latent representation.
This is a classic SSL task and has been applied in GNN systems with success. This can be thought of as a smaller version of full molecular reconstruction. If the model is given some chemical or structural descriptors of a molecule, and an incomplete molecular graph, by predicting the masked part of the graph, it can learn HOW each piece of a molecular graph impacts its structural and chemical properties.

**Differential Coarseness Cluster Contrastive Learning (and cluster prediction)**. This pretraining task is not entirely new, and some variations of it have already been applied to the problem of graph representation learning. The idea is to take some representation of every molecule, it could be the actual graph or its node matrix, but it is probably more realistic to use a structural fingerprint. This representation is clustered with kNN or a similar algorithm at different coarseness (maybe k=100, 1,000, and 10,000). Given a latent representation of a molecular graph, a decoder would aim to predict which neighborhood a molecule belongs to (cluster prediction). Next, contrastive learning is applied, molecules who share some of their top 5 or so neighborhoods would be treated as similar, while those who do not are treated as dissimilar. These labels would then be used as the basis of contrastive learning, a loss function would enforce that molecules in the same neighborhood have similar latent representations. I haven’t implemented code for this yet, but through some experiments, I’ve found that Faiss can dramatically speed up kNN with GPU acceleration.
The main idea behind this task is getting the model to understand how different pieces of chemical or structural knowledge are related. By predicting which cluster each molecule belongs to, the aim is to further the understanding of how different molecules relate to one another in chemical feature space. By predicting k clusters and not just the raw chemical attributes, the model can learn which molecular descriptors separate a molecule in latent space.
A core paradigm that has emerged recently in NLP is the idea of multitask pretraining. Part of the motivation behind any given pre-training task is to increase the diversity of the tasks themselves. Models pretrained on many tasks have been shown to generalize better to new tasks in NLP and we hope to generalize this idea to GNNs, in which most pre-trained molecular property prediction models do not use multi task pretraining.
